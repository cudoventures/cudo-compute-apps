services:
  triton:
    user: triton-server
    image: nvcr.io/nvidia/tritonserver:${TRITON_IMAGE}
    volumes:
      - /root/models:/models
    networks:
      - cudo-net
    environment:
      - TRITON_IMAGE=25.01-py3 #25.01-vllm-python-py3
      - TRITON_MODEL_CONTROL_MODE=poll # or use 'explicit' and curl -X POST http://<triton-server-url>/v2/repository/models/<model_name>/load
    command: [ "tritonserver", "--model-store=/models", "--model-control-mode=${TRITON_MODEL_CONTROL_MODE}" ]
    shm_size: 2g  # Set shared memory size to 2GB
    ulimits:
      memlock: -1  # Lock memory to unlimited
      stack: 67108864  # Set stack size to 64MB
    ports:
      - "127.0.0.1:8080:8000" # HTTP
      - "127.0.0.1:8081:8001" # GRPC
      - "127.0.0.1:8082:8002" # Metrics
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]
  nginx:
    image: nginx
    container_name: nginx-auth
    depends_on:
      - triton
    ports:
      - "8000:8000"
      - "8001:8001"
      - "8002:8002"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
    environment:
      - AUTH_TOKEN=set_in_env
    networks:
      - cudo-net

networks:
  cudo-net:
   driver: bridge
